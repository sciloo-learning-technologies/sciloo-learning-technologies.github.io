<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>healthcare &#8211; Sciloo Learning Technologies</title>
	<atom:link href="http://localhost/tag/healthcare/feed/" rel="self" type="application/rss+xml" />
	<link>http://localhost:8180</link>
	<description>simplifying innovation</description>
	<lastBuildDate>Sun, 17 Feb 2019 08:34:46 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Model-Based Machine Learning</title>
		<link>http://localhost:8180/2019/02/15/model-based-learning/</link>
					<comments>http://localhost:8180/2019/02/15/model-based-learning/#respond</comments>
		
		<dc:creator><![CDATA[admin@sciloo.com]]></dc:creator>
		<pubDate>Fri, 15 Feb 2019 18:07:44 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[technology]]></category>
		<category><![CDATA[healthcare]]></category>
		<category><![CDATA[machine learning]]></category>
		<guid isPermaLink="false">https://sciloo.com/?p=1059</guid>

					<description><![CDATA[ML not equal to data + black box&#160; ML = data + assumptions&#160;&#160; Free Lunch Theorem&#160; As per free lunch theorem, without assumptions we can’t build anything.&#160;We can’t have one generic&#160;algo&#160;that is applicable for all the problems.&#160;&#160; We have&#160;constraints&#160;like&#160;&#160; &#160; assumptions,&#160;&#160; &#160; prior knowledge,&#160;&#160; &#160; domain knowledge&#160; &#160;when dealing with problems and without considering these&#8230; <a class="more-link" href="http://localhost:8180/2019/02/15/model-based-learning/">Continue reading <span class="screen-reader-text">Model-Based Machine Learning</span></a>]]></description>
										<content:encoded><![CDATA[
<p>ML not equal to data + black box&nbsp;</p>



<p>ML = data + assumptions&nbsp;&nbsp;</p>



<p><strong>Free Lunch Theorem</strong>&nbsp;</p>



<p>As per free lunch theorem, without assumptions we can’t build anything.&nbsp;We can’t have one generic&nbsp;<g class="gr_ gr_4 gr-alert gr_spell gr_inline_cards gr_run_anim ContextualSpelling ins-del multiReplace" id="4" data-gr-id="4">algo</g>&nbsp;that is applicable for all the problems.&nbsp;&nbsp;</p>



<p>We have&nbsp;<strong>constraints&nbsp;</strong>like&nbsp;&nbsp;</p>



<p>&nbsp; assumptions,&nbsp;&nbsp;</p>



<p>&nbsp; prior knowledge,&nbsp;&nbsp;</p>



<p>&nbsp; domain knowledge&nbsp;</p>



<p>&nbsp;when dealing with problems and without considering these how we can solve a problem.&nbsp;</p>



<p><strong>Data + prior knowledge</strong>&nbsp;</p>



<p>Detection of a person in frame&nbsp;</p>



<p>In a simple approach, we will need many images with person at different location.&nbsp;</p>



<p>But with this knowledge, we can develop transformations to shift a person in the image and then train the model on these images instead of acquiring those images.&nbsp;</p>



<p>We tend to think about Big data that we can develop weak models and can forget the constraints/assumptions.&nbsp;</p>



<p>There are 2 types of sizes&nbsp;</p>



<p>&nbsp; 1. computation size&nbsp;</p>



<p>&nbsp; 2. statistical size&nbsp;</p>



<div class="wp-block-image"><figure class="alignleft is-resized"><img loading="lazy" src="http://localhost:8180/wp-content/uploads/2019/02/image.png" alt="" class="wp-image-1060" width="243" height="198"/></figure></div>



<figure class="wp-block-image is-resized"><img loading="lazy" src="http://localhost:8180/wp-content/uploads/2019/02/image-1.png" alt="" class="wp-image-1061" width="236" height="195"/></figure>



<p></p>



<p>Computationally small but statistical big&nbsp;</p>



<p>Without assumptions, how we can predict a current value for a new voltage value?&nbsp; On the right side, we can take into consideration about V = IR and can be confident that the line is good predictor for future voltage values. This dataset is small in computational size but big in statistical size.&nbsp;</p>



<figure class="wp-block-image is-resized"><img loading="lazy" src="http://localhost:8180/wp-content/uploads/2019/02/image-3.png" alt="" class="wp-image-1063" width="384" height="261"/></figure>



<p>Statistically small but computationally big&nbsp;</p>



<p>The above data is computationally big, take a simple example of a <g class="gr_ gr_5 gr-alert gr_gramm gr_inline_cards gr_run_anim Grammar only-ins doubleReplace replaceWithoutSep" id="5" data-gr-id="5">black</g> and white image of 10X10 pixel, all the combinations can yield combinations that might be more than the total number of atoms in the universe.&nbsp;</p>



<p>Unless we make assumptions, it is very difficult to do computation with these big set of images. This above images data can be computationally very large but statistically very small.&nbsp;</p>



<p><strong>Model based ML</strong>&nbsp;</p>



<p>Instead of getting lost in the sea of algorithms/models/research papers, we should use model-based learning to use data and assumptions to derive ML algorithm.&nbsp;We should work mainly on defining our assumptions about the problem very clearly.&nbsp;&nbsp;</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Machine learning and the learning machine with Dr. Christopher Bishop" width="750" height="422" src="https://www.youtube.com/embed/9W2BcQyT0ZM?start=695&#038;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></figure>



<p>Translate your assumptions into a model and that is the core of model-based learning. If our assumptions are sound/strong then we get much more information from the same amount of data.&nbsp;</p>



<p>Can we develop a tool to do it automatically? Can this dream be true?&nbsp;</p>



<figure class="wp-block-image is-resized"><img loading="lazy" src="http://localhost:8180/wp-content/uploads/2019/02/image-4.png" alt="" class="wp-image-1064" width="288" height="235"/></figure>



<p><strong>User &amp; Movies</strong>&nbsp;</p>



<p>Predict the question marks – whether a user like or dislike&nbsp;those movie&nbsp;for which he has not given&nbsp;a&nbsp;answer. 10ks of movies, 10Ms of users.&nbsp;</p>



<figure class="wp-block-image is-resized"><img loading="lazy" src="http://localhost:8180/wp-content/uploads/2019/02/image-2.png" alt="" class="wp-image-1062" width="433" height="331"/></figure>



<figure class="wp-block-image is-resized"><img loading="lazy" src="http://localhost:8180/wp-content/uploads/2019/02/image-5-1024x507.png" alt="" class="wp-image-1065" width="618" height="306"/></figure>



<p>Based on my like or dislike, ML is suggesting movies closer to green area that I can like for sure and closer to red area for movies that I may not like.&nbsp;</p>



<p>It is learning my behavior and suggesting me titles based on the populations&nbsp;similar to&nbsp;me.&nbsp;</p>



<p>Information (as per information theory by&nbsp;Shanon) is the surprise from data. Just like above movie ML&nbsp;algo&nbsp;suggest me movies closer to green area and I pick then there is a very less surprise in the data and suggestions don’t move big.&nbsp;</p>



<p>But instead of liking the movie, if I dislike it, it is a big surprise for the&nbsp;algo&nbsp;and movies suggestions move big.&nbsp;</p>



<p><strong>Quantifying uncertainty</strong>&nbsp;</p>



<p>Uncertainty is an essential part of machine learning and it is a modern view of ML.&nbsp;<strong>Bayes theorem</strong>&nbsp;is at the core of it.&nbsp;</p>



<p>BT: P(A/B) = [P(A) * P(B/A)]/P(B)&nbsp;</p>



<p><strong>Bishop view on Healthcare</strong>&nbsp;</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Machine learning and the learning machine with Dr. Christopher Bishop" width="750" height="422" src="https://www.youtube.com/embed/9W2BcQyT0ZM?start=695&#038;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></figure>



<p>It is one of the biggest opportunities but complex also. Personalized healthcare can be a phenomenon suggesting you personalized course of treatment because as an individual each one of us is unique. It is like machine can learn our movies preferences.&nbsp;</p>



<p>&nbsp;This can happen by analyzing the info from other millions of people.&nbsp;&nbsp;</p>



<p>&nbsp; Cloud based tech + ML tech + Domain experts (from Hospitals like clinicians, doctors, radiologists, etc.)&nbsp;</p>



<p><strong>Trust and privacy in the Data</strong>&nbsp;</p>



<p>Data is encrypted so it is only accessible to those who have keys but It can become vulnerable while being processed. So Microsoft has taken care of processing data in <g class="gr_ gr_7 gr-alert gr_gramm gr_inline_cards gr_run_anim Grammar only-ins replaceWithoutSep" id="7" data-gr-id="7">secure</g> vault kind of infrastructure where data can be <g class="gr_ gr_10 gr-alert gr_spell gr_inline_cards gr_run_anim ContextualSpelling" id="10" data-gr-id="10">access</g> only to people who have keys. It is secure by hardware and software. </p>



<p><strong>Factor Graph</strong></p>



<figure class="wp-block-image"><img src="http://localhost:8180/wp-content/uploads/2019/02/image-18.png" alt="" class="wp-image-1120"/></figure>



<p>The probability of getting a circular or triangular cookie depends upon which jar we choose. Round circles on the right sides are &#8211; one random variable for Jar, one random variable for Cookie</p>



<p>These random variables are driven by their respective probability distribution functions (PDFs) denoted as shaded squares.</p>
]]></content:encoded>
					
					<wfw:commentRss>http://localhost:8180/2019/02/15/model-based-learning/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
